{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84331c10",
   "metadata": {},
   "source": [
    "# MyUpchar blogs extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4580b",
   "metadata": {},
   "source": [
    "## 1. Article scrapper code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3acdc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json, os\n",
    "\n",
    "\n",
    "def clean_text_with_links(tag):\n",
    "    \"\"\"Rebuild paragraph text while preserving <a> tag text with space.\"\"\"\n",
    "    parts = []\n",
    "    for child in tag.descendants:\n",
    "        if child.name == \"a\":\n",
    "            text = child.get_text(strip=True)\n",
    "            parts.append(text)\n",
    "        elif isinstance(child, str):\n",
    "            parts.append(child)\n",
    "    return \"\".join(parts).strip()\n",
    "\n",
    "\n",
    "def extract_intro_content(content_body_div):\n",
    "    \"\"\"Extracts the content before sections (main body / intro).\"\"\"\n",
    "    intro = []\n",
    "    for child in content_body_div.children:\n",
    "        if getattr(child, \"name\", None) == \"p\":\n",
    "            strong = child.find(\"strong\")\n",
    "            if strong and strong.text.strip() == child.text.strip():\n",
    "                intro.append(\"### \" + strong.get_text(strip=True))\n",
    "            else:\n",
    "                intro.append(clean_text_with_links(child))\n",
    "        elif getattr(child, \"name\", None) == \"ul\":\n",
    "            for li in child.find_all(\"li\"):\n",
    "                intro.append(\"‚Ä¢ \" + clean_text_with_links(li))\n",
    "    return intro\n",
    "\n",
    "\n",
    "def scrape_myupchar_article(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"No Title\"\n",
    "    content = {\"url\": url, \"title\": title, \"intro\": [], \"sections\": []}\n",
    "\n",
    "    # === Extract Main Body (before sections)\n",
    "    intro_div = soup.find(\"div\", id=\"content-body\")\n",
    "    if intro_div:\n",
    "        content[\"intro\"] = extract_intro_content(intro_div)\n",
    "\n",
    "    # === Extract Structured Sections\n",
    "    for section in soup.select(\".category_blue_backdiv\"):\n",
    "        heading_tag = section.find(\"h2\")\n",
    "        body_tag = section.find(\"div\", class_=\"description\")\n",
    "\n",
    "        section_title = (\n",
    "            heading_tag.get_text(strip=True) if heading_tag else \"No Heading\"\n",
    "        )\n",
    "        section_text = []\n",
    "\n",
    "        if body_tag:\n",
    "            for child in body_tag.children:\n",
    "                if child.name == \"p\":\n",
    "                    strong = child.find(\"strong\")\n",
    "                    if strong and strong.text.strip() == child.text.strip():\n",
    "                        section_text.append(\"### \" + strong.get_text(strip=True))\n",
    "                    else:\n",
    "                        section_text.append(clean_text_with_links(child))\n",
    "                elif child.name == \"ul\":\n",
    "                    for li in child.find_all(\"li\"):\n",
    "                        section_text.append(\"‚Ä¢ \" + clean_text_with_links(li))\n",
    "                elif child.name == \"div\":\n",
    "                    for subchild in child.children:\n",
    "                        if subchild.name == \"p\":\n",
    "                            section_text.append(clean_text_with_links(subchild))\n",
    "                        elif subchild.name == \"ul\":\n",
    "                            for li in subchild.find_all(\"li\"):\n",
    "                                section_text.append(\"‚Ä¢ \" + clean_text_with_links(li))\n",
    "\n",
    "        content[\"sections\"].append({\"heading\": section_title, \"body\": section_text})\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab83d1f",
   "metadata": {},
   "source": [
    "### 1.1. Save as Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e368ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_markdown(article_data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as md:\n",
    "        md.write(f\"# {article_data['title']}\\n\")\n",
    "        md.write(f\"_Source: [{article_data['url']}]({article_data['url']})_\\n\\n\")\n",
    "\n",
    "        if article_data[\"intro\"]:\n",
    "            md.write(\"## Introduction\\n\")\n",
    "            for line in article_data[\"intro\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"‚Ä¢ \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")\n",
    "\n",
    "        for sec in article_data[\"sections\"]:\n",
    "            md.write(f\"## {sec['heading']}\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"‚Ä¢ \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600c800",
   "metadata": {},
   "source": [
    "## 2. Bulk Article URL Scrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19abbc2",
   "metadata": {},
   "source": [
    "#### Tool 1: Extract Links from Category page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c70a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 21 article URLs from <h4> tags.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_blog_urls(html_path, base_url=\"https://www.myupchar.com\"):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(html_path, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    for h4 in soup.find_all(\"h4\"):\n",
    "        a_tag = h4.find(\"a\", href=True)\n",
    "        if a_tag:\n",
    "            href = a_tag[\"href\"]\n",
    "            links.add(base_url + href.strip())\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "urls = extract_blog_urls(input(\"Paste category page URL here: \"))\n",
    "\n",
    "with open(\"myupchar_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Extracted {len(urls)} article URLs from <h4> tags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7091bf",
   "metadata": {},
   "source": [
    "#### Tool 2: Bulk Scrap articles from saved links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d14cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/21] Scraping: https://www.myupchar.com/pregnancy/which-side-is-baby-boy-in-womb\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/which-side-is-baby-boy-in-womb.txt\n",
      "  JSON: jsonblogs/pregnancy/which-side-is-baby-boy-in-womb.json\n",
      "  MD: markdownblogs/pregnancy/which-side-is-baby-boy-in-womb.md\n",
      "‚úÖ Success: ‡§™‡•ç‡§∞‡•á‡§ó‡§®‡•á‡§Ç‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§¨‡•á‡§¨‡•Ä ‡§¨‡•â‡§Ø ‡§ï‡§ø‡§∏ ‡§∏‡§æ‡§á‡§° ‡§π‡•ã‡§§‡§æ ‡§π‡•à? - Which side is baby boy side in womb in Hindi\n",
      "[2/21] Scraping: https://www.myupchar.com/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe.txt\n",
      "  JSON: jsonblogs/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe.json\n",
      "  MD: markdownblogs/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe.md\n",
      "‚úÖ Success: ‡§ï‡•ç‡§Ø‡§æ ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ ‡§Æ‡•á‡§Ç ‡§π‡§∏‡•ç‡§§‡§Æ‡•à‡§•‡•Å‡§® ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç? - Is masturbation during pregnancy safe in Hindi\n",
      "[3/21] Scraping: https://www.myupchar.com/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy.md\n",
      "‚úÖ Success: ‡§ó‡§∞‡•ç‡§≠‡§µ‡§§‡•Ä ‡§Æ‡§π‡§ø‡§≤‡§æ 8‡§µ‡•á‡§Ç ‡§Æ‡§π‡•Ä‡§®‡•á ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§ñ‡§æ‡§è? - What Should Eat In Eighth Month Pregnancy In Hindi\n",
      "[4/21] Scraping: https://www.myupchar.com/pregnancy/can-you-get-pregnant-without-having-sex\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/can-you-get-pregnant-without-having-sex.txt\n",
      "  JSON: jsonblogs/pregnancy/can-you-get-pregnant-without-having-sex.json\n",
      "  MD: markdownblogs/pregnancy/can-you-get-pregnant-without-having-sex.md\n",
      "‚úÖ Success: ‡§ï‡•ç‡§Ø‡§æ ‡§¨‡§ø‡§®‡§æ ‡§∏‡•á‡§ï‡•ç‡§∏ ‡§ï‡•á ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ ‡§∏‡§Ç‡§≠‡§µ ‡§π‡•à? - Can You Get Pregnant Without Having Sex In Hindi\n",
      "[5/21] Scraping: https://www.myupchar.com/pregnancy/effects-of-lack-of-sleep-during-pregnancy\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/effects-of-lack-of-sleep-during-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/effects-of-lack-of-sleep-during-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/effects-of-lack-of-sleep-during-pregnancy.md\n",
      "‚úÖ Success: ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡§Æ ‡§∏‡•ã‡§®‡•á ‡§ï‡•á ‡§¶‡•Å‡§∑‡•ç‡§™‡•ç‡§∞‡§≠‡§æ‡§µ - Effects Of Lack Of Sleep During Pregnancy In Hindi\n",
      "[6/21] Scraping: https://www.myupchar.com/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms.txt\n",
      "  JSON: jsonblogs/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms.json\n",
      "  MD: markdownblogs/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms.md\n",
      "‚úÖ Success: ‡§Æ‡§∏‡•ç‡§ï‡•Å‡§≤‡§∞ ‡§°‡§ø‡§∏‡•ç‡§ü‡•ç‡§∞‡•â‡§´‡•Ä ‡§î‡§∞ ‡§™‡•ç‡§∞‡•á‡§ó‡§®‡•á‡§Ç‡§∏‡•Ä  - Muscular Dystrophy And Pregnancy In Hindi\n",
      "[7/21] Scraping: https://www.myupchar.com/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth.txt\n",
      "  JSON: jsonblogs/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth.json\n",
      "  MD: markdownblogs/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth.md\n",
      "‚úÖ Success: ‡§ï‡§ø‡§§‡§®‡•á ‡§π‡§´‡•ç‡§§‡•á ‡§Æ‡•á‡§Ç ‡§°‡§ø‡§≤‡•Ä‡§µ‡§∞‡•Ä ‡§π‡•ã‡§®‡•Ä ‡§ö‡§æ‡§π‡§ø‡§è? - At how many weeks can you safely give birth in Hindi\n",
      "[8/21] Scraping: https://www.myupchar.com/pregnancy/vacuum-assisted-delivery\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/vacuum-assisted-delivery.txt\n",
      "  JSON: jsonblogs/pregnancy/vacuum-assisted-delivery.json\n",
      "  MD: markdownblogs/pregnancy/vacuum-assisted-delivery.md\n",
      "‚úÖ Success: ‡§µ‡•à‡§ï‡•ç‡§Ø‡•Ç‡§Æ ‡§Ö‡§∏‡§ø‡§∏‡•ç‡§ü‡•á‡§° ‡§°‡§ø‡§≤‡•Ä‡§µ‡§∞‡•Ä - Know About Vacuum Assisted Delivery In Hindi\n",
      "[9/21] Scraping: https://www.myupchar.com/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein.txt\n",
      "  JSON: jsonblogs/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein.json\n",
      "  MD: markdownblogs/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein.md\n",
      "‚úÖ Success: ‡§°‡§ø‡§≤‡•Ä‡§µ‡§∞‡•Ä ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§™‡•á‡§ü ‡§ï‡§æ ‡§ï‡§æ‡§≤‡§æ‡§™‡§® ‡§ï‡•à‡§∏‡•á ‡§¶‡•Ç‡§∞ ‡§ï‡§∞‡•á‡§Ç? - how to get rid of blackness on stomach after delivery in hindi\n",
      "[10/21] Scraping: https://www.myupchar.com/pregnancy/pregnancy-brain\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/pregnancy-brain.txt\n",
      "  JSON: jsonblogs/pregnancy/pregnancy-brain.json\n",
      "  MD: markdownblogs/pregnancy/pregnancy-brain.md\n",
      "‚úÖ Success: ‡§™‡•ç‡§∞‡•á‡§ó‡§®‡•á‡§Ç‡§∏‡•Ä ‡§¨‡•ç‡§∞‡•á‡§® : ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à, ‡§≤‡§ï‡•ç‡§∑‡§£, ‡§ï‡§æ‡§∞‡§£ ‡§µ ‡§∏‡§Æ‡§æ‡§ß‡§æ‡§® - Pregnancy Brain : What Is It, Symptoms, Causes And Treatment In Hindi\n",
      "[11/21] Scraping: https://www.myupchar.com/pregnancy/heart-diseases\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/heart-diseases.txt\n",
      "  JSON: jsonblogs/pregnancy/heart-diseases.json\n",
      "  MD: markdownblogs/pregnancy/heart-diseases.md\n",
      "‚úÖ Success: ‡§™‡•ç‡§∞‡•á‡§ó‡§®‡•á‡§Ç‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§π‡•ã‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§π‡•É‡§¶‡§Ø ‡§∞‡•ã‡§ó ‡§µ ‡§≤‡§ï‡•ç‡§∑‡§£ - Heart Diseases In Pregnancy And Symptoms In Hindi\n",
      "[12/21] Scraping: https://www.myupchar.com/pregnancy/how-to-prevent-blood-clots\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/how-to-prevent-blood-clots.txt\n",
      "  JSON: jsonblogs/pregnancy/how-to-prevent-blood-clots.json\n",
      "  MD: markdownblogs/pregnancy/how-to-prevent-blood-clots.md\n",
      "‚úÖ Success: ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ ‡§Æ‡•á‡§Ç ‡§ñ‡•Ç‡§® ‡§ï‡•á ‡§•‡§ï‡•ç‡§ï‡•á ‡§¨‡§®‡§®‡•á ‡§∏‡•á ‡§ï‡•à‡§∏‡•á ‡§∞‡•ã‡§ï‡•á‡§Ç? - How To Prevent Blood Clots In Pregnancy In Hindi\n",
      "[13/21] Scraping: https://www.myupchar.com/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant.txt\n",
      "  JSON: jsonblogs/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant.json\n",
      "  MD: markdownblogs/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant.md\n",
      "‚úÖ Success: ‡§™‡•ç‡§∞‡•á‡§ó‡•ç‡§®‡•á‡§Ç‡§ü ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§¨‡•á‡§∏‡•ç‡§ü ‡§∏‡•á‡§ï‡•ç‡§∏ ‡§™‡•ã‡§ú‡•Ä‡§∂‡§® - Best sex position to get pregnant in Hindi\n",
      "[14/21] Scraping: https://www.myupchar.com/women-health/copper-t-in-hindi\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/women-health/copper-t-in-hindi.txt\n",
      "  JSON: jsonblogs/women-health/copper-t-in-hindi.json\n",
      "  MD: markdownblogs/women-health/copper-t-in-hindi.md\n",
      "‚úÖ Success: ‡§ï‡•â‡§™‡§∞ ‡§ü‡•Ä ‡§ï‡•ç‡§Ø‡•ã‡§Ç, ‡§ï‡•à‡§∏‡•á ‡§≤‡§ó‡§§‡•Ä ‡§π‡•à, ‡§®‡§ø‡§ï‡§≤‡§®‡•á ‡§ï‡§æ ‡§§‡§∞‡•Ä‡§ï‡§æ, ‡§´‡§æ‡§Ø‡§¶‡•á ‡§î‡§∞ ‡§®‡•Å‡§ï‡§∏‡§æ‡§® - Copper T In Hindi\n",
      "[15/21] Scraping: https://www.myupchar.com/pregnancy/anembryonic-pregnancy\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/anembryonic-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/anembryonic-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/anembryonic-pregnancy.md\n",
      "‚úÖ Success: ‡§è‡§®‡§¨‡•ç‡§∞‡§æ‡§Ø‡•ã‡§®‡§ø‡§ï ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ - Anembryonic Pregnancy in Hindi\n",
      "[16/21] Scraping: https://www.myupchar.com/pregnancy/prega-news-in-hindi\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/prega-news-in-hindi.txt\n",
      "  JSON: jsonblogs/pregnancy/prega-news-in-hindi.json\n",
      "  MD: markdownblogs/pregnancy/prega-news-in-hindi.md\n",
      "‚úÖ Success: ‡§™‡•ç‡§∞‡•á‡§ó‡§æ ‡§®‡•ç‡§Ø‡•Ç‡§ú‡§º ‡§ï‡§ø‡§ü ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡•à‡§∏‡•á ‡§ï‡§∞‡•á‡§Ç? - How To Use Prega News Pregnancy Test Kit In Hindi\n",
      "[17/21] Scraping: https://www.myupchar.com/pregnancy/diet/watermelon-for-pregnancy\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/diet/watermelon-for-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/diet/watermelon-for-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/diet/watermelon-for-pregnancy.md\n",
      "‚úÖ Success: ‡§™‡•ç‡§∞‡•á‡§ó‡•ç‡§®‡•á‡§Ç‡§∏‡•Ä ‡§Æ‡•á‡§Ç ‡§§‡§∞‡§¨‡•Ç‡§ú ‡§ñ‡§æ‡§®‡•á ‡§ï‡•á ‡§´‡§æ‡§Ø‡§¶‡•á  - Watermelon Benefits For Pregnancy in Hindi\n",
      "[18/21] Scraping: https://www.myupchar.com/surgery/c-section-cesarean-section/safe-sex-position-after-c-section\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/surgery/c-section-cesarean-section/safe-sex-position-after-c-section.txt\n",
      "  JSON: jsonblogs/surgery/c-section-cesarean-section/safe-sex-position-after-c-section.json\n",
      "  MD: markdownblogs/surgery/c-section-cesarean-section/safe-sex-position-after-c-section.md\n",
      "‚úÖ Success: ‡§∏‡•Ä-‡§∏‡•á‡§ï‡•ç‡§∂‡§® ‡§ï‡•á ‡§¨‡§æ‡§¶ ‡§∏‡•á‡§´ ‡§∏‡•á‡§ï‡•ç‡§∏ ‡§™‡•ã‡§ú‡•Ä‡§∂‡§® - Safe Sex Position After Cesarean Delivery In Hindi\n",
      "[19/21] Scraping: https://www.myupchar.com/pregnancy/pregnancy-hip-pain\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/pregnancy-hip-pain.txt\n",
      "  JSON: jsonblogs/pregnancy/pregnancy-hip-pain.json\n",
      "  MD: markdownblogs/pregnancy/pregnancy-hip-pain.md\n",
      "‚úÖ Success: ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‚Äç‡§•‡§æ ‡§Æ‡•á‡§Ç ‡§ï‡•Ç‡§≤‡•ç‡§π‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§¶‡§∞‡•ç‡§¶ - Hip Pain During Pregnancy in Hindi\n",
      "[20/21] Scraping: https://www.myupchar.com/pregnancy/when-a-baby-drops-during-pregnancy\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/when-a-baby-drops-during-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/when-a-baby-drops-during-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/when-a-baby-drops-during-pregnancy.md\n",
      "‚úÖ Success: ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ ‡§Æ‡•á‡§Ç ‡§∂‡§ø‡§∂‡•Å ‡§®‡•Ä‡§ö‡•á ‡§π‡•ã ‡§§‡•ã ‡§ï‡•ç‡§Ø‡§æ ‡§ï‡§∞‡•á‡§Ç? - When A Baby Drops During Pregnancy In Hindi\n",
      "[21/21] Scraping: https://www.myupchar.com/pregnancy/diet/probiotics-for-pregnancy\n",
      "‚úÖ Saved:\n",
      "  TXT: blogs/pregnancy/diet/probiotics-for-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/diet/probiotics-for-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/diet/probiotics-for-pregnancy.md\n",
      "‚úÖ Success: ‡§ï‡•ç‡§Ø‡§æ ‡§ó‡§∞‡•ç‡§≠‡§æ‡§µ‡§∏‡•ç‡§•‡§æ ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§™‡•ç‡§∞‡•ã‡§¨‡§æ‡§Ø‡•ã‡§ü‡§ø‡§ï‡•ç‡§∏ ‡§≤‡•á‡§®‡§æ ‡§ö‡§æ‡§π‡§ø‡§è? - Should Probiotics Be Taken During Pregnancy in Hindi\n",
      "üìö Master index saved to 'master_index.json'\n",
      "\n",
      "üü¢ Finished: 21 success, 0 failed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Reuse the existing scrape_myupchar_article() and save_as_markdown() functions\n",
    "# Make sure those are defined/imported in this script\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def safe_path_from_url(url):\n",
    "    \"\"\"Convert the URL path into a folder path (no leading slash).\"\"\"\n",
    "    path = url.replace(\"https://www.myupchar.com/\", \"\").strip(\"/\")\n",
    "    return path  # e.g., \"en/disease/acidity\"\n",
    "\n",
    "\n",
    "def save_all_formats(article):\n",
    "    relative_path = safe_path_from_url(article[\"url\"])  # e.g., en/disease/acidity\n",
    "    base_name = os.path.basename(relative_path)  # e.g., acidity\n",
    "\n",
    "    # Paths\n",
    "    txt_path = os.path.join(\"blogs\", relative_path + \".txt\")\n",
    "    json_path = os.path.join(\"jsonblogs\", relative_path + \".json\")\n",
    "    md_path = os.path.join(\"markdownblogs\", relative_path + \".md\")\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(os.path.dirname(txt_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(md_path), exist_ok=True)\n",
    "\n",
    "    # Save as .txt\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Title: {article['title']}\\n\")\n",
    "        f.write(f\"URL: {article['url']}\\n\\n\")\n",
    "        f.write(\"== Introduction ==\\n\")\n",
    "        for line in article[\"intro\"]:\n",
    "            f.write(f\"{line}\\n\")\n",
    "        for sec in article[\"sections\"]:\n",
    "            f.write(f\"\\n== {sec['heading']} ==\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    # Save as .json\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Save as .md\n",
    "    save_as_markdown(article, md_path)\n",
    "\n",
    "    print(f\"‚úÖ Saved:\\n  TXT: {txt_path}\\n  JSON: {json_path}\\n  MD: {md_path}\")\n",
    "\n",
    "\n",
    "def bulk_scrape(url_list, delay_range=(2, 5)):\n",
    "    failed_urls = []\n",
    "    success_index = []\n",
    "\n",
    "    for idx, url in enumerate(url_list, start=1):\n",
    "        print(f\"[{idx}/{len(url_list)}] Scraping: {url}\")\n",
    "\n",
    "        try:\n",
    "            # Use URL path to construct relative storage path\n",
    "            relative_path = safe_path_from_url(url)  # e.g., en/disease/acidity\n",
    "            base_name = os.path.basename(relative_path)\n",
    "\n",
    "            json_path = os.path.join(\"jsonblogs\", relative_path + \".json\")\n",
    "\n",
    "            # ‚úÖ Skip early if already exists\n",
    "            if os.path.exists(json_path):\n",
    "                print(f\"‚ö†Ô∏è Skipped (already scraped): {relative_path}\")\n",
    "                continue\n",
    "\n",
    "            article = scrape_myupchar_article(url)\n",
    "            save_all_formats(article)\n",
    "            print(f\"‚úÖ Success: {article['title']}\")\n",
    "\n",
    "            success_index.append(\n",
    "                {\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"slug\": base_name,\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"path\": relative_path,\n",
    "                    \"files\": {\n",
    "                        \"txt\": f\"blogs/{relative_path}.txt\",\n",
    "                        \"json\": f\"jsonblogs/{relative_path}.json\",\n",
    "                        \"md\": f\"markdownblogs/{relative_path}.md\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # ‚úÖ Only wait if we actually scraped\n",
    "            time.sleep(random.uniform(*delay_range))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed: {url}\\n   Reason: {e}\")\n",
    "            failed_urls.append(url)\n",
    "\n",
    "    # Save failed URLs\n",
    "    if failed_urls:\n",
    "        with open(\"failed_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(url + \"\\n\" for url in failed_urls)\n",
    "        print(\"‚ö†Ô∏è Failed URLs saved to 'failed_urls.txt'\")\n",
    "\n",
    "    # Save master index\n",
    "    with open(\"master_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(success_index, f, indent=4, ensure_ascii=False)\n",
    "    print(\"üìö Master index saved to 'master_index.json'\")\n",
    "\n",
    "    print(f\"\\nüü¢ Finished: {len(success_index)} success, {len(failed_urls)} failed.\")\n",
    "\n",
    "\n",
    "# Load main list of URLs\n",
    "with open(\"myupchar_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Load failed URLs (if resume is enabled)\n",
    "if os.path.exists(\"failed_urls.txt\"):\n",
    "    with open(\"failed_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        failed = [line.strip() for line in f if line.strip()]\n",
    "    all_urls.extend(failed)\n",
    "\n",
    "# Remove duplicates\n",
    "all_urls = list(set(all_urls))\n",
    "\n",
    "# Start scraper\n",
    "bulk_scrape(all_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
