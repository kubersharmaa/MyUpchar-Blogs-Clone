{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84331c10",
   "metadata": {},
   "source": [
    "# MyUpchar blogs extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4580b",
   "metadata": {},
   "source": [
    "## 1. Article scrapper code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3acdc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json, os\n",
    "\n",
    "\n",
    "def clean_text_with_links(tag):\n",
    "    \"\"\"Rebuild paragraph text while preserving <a> tag text with space.\"\"\"\n",
    "    parts = []\n",
    "    for child in tag.descendants:\n",
    "        if child.name == \"a\":\n",
    "            text = child.get_text(strip=True)\n",
    "            parts.append(text)\n",
    "        elif isinstance(child, str):\n",
    "            parts.append(child)\n",
    "    return \"\".join(parts).strip()\n",
    "\n",
    "\n",
    "def extract_intro_content(content_body_div):\n",
    "    \"\"\"Extracts the content before sections (main body / intro).\"\"\"\n",
    "    intro = []\n",
    "    for child in content_body_div.children:\n",
    "        if getattr(child, \"name\", None) == \"p\":\n",
    "            strong = child.find(\"strong\")\n",
    "            if strong and strong.text.strip() == child.text.strip():\n",
    "                intro.append(\"### \" + strong.get_text(strip=True))\n",
    "            else:\n",
    "                intro.append(clean_text_with_links(child))\n",
    "        elif getattr(child, \"name\", None) == \"ul\":\n",
    "            for li in child.find_all(\"li\"):\n",
    "                intro.append(\"• \" + clean_text_with_links(li))\n",
    "    return intro\n",
    "\n",
    "\n",
    "def scrape_myupchar_article(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"No Title\"\n",
    "    content = {\"url\": url, \"title\": title, \"intro\": [], \"sections\": []}\n",
    "\n",
    "    # === Extract Main Body (before sections)\n",
    "    intro_div = soup.find(\"div\", id=\"content-body\")\n",
    "    if intro_div:\n",
    "        content[\"intro\"] = extract_intro_content(intro_div)\n",
    "\n",
    "    # === Extract Structured Sections\n",
    "    for section in soup.select(\".category_blue_backdiv\"):\n",
    "        heading_tag = section.find(\"h2\")\n",
    "        body_tag = section.find(\"div\", class_=\"description\")\n",
    "\n",
    "        section_title = (\n",
    "            heading_tag.get_text(strip=True) if heading_tag else \"No Heading\"\n",
    "        )\n",
    "        section_text = []\n",
    "\n",
    "        if body_tag:\n",
    "            for child in body_tag.children:\n",
    "                if child.name == \"p\":\n",
    "                    strong = child.find(\"strong\")\n",
    "                    if strong and strong.text.strip() == child.text.strip():\n",
    "                        section_text.append(\"### \" + strong.get_text(strip=True))\n",
    "                    else:\n",
    "                        section_text.append(clean_text_with_links(child))\n",
    "                elif child.name == \"ul\":\n",
    "                    for li in child.find_all(\"li\"):\n",
    "                        section_text.append(\"• \" + clean_text_with_links(li))\n",
    "                elif child.name == \"div\":\n",
    "                    for subchild in child.children:\n",
    "                        if subchild.name == \"p\":\n",
    "                            section_text.append(clean_text_with_links(subchild))\n",
    "                        elif subchild.name == \"ul\":\n",
    "                            for li in subchild.find_all(\"li\"):\n",
    "                                section_text.append(\"• \" + clean_text_with_links(li))\n",
    "\n",
    "        content[\"sections\"].append({\"heading\": section_title, \"body\": section_text})\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab83d1f",
   "metadata": {},
   "source": [
    "### 1.1. Save as Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e368ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_markdown(article_data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as md:\n",
    "        md.write(f\"# {article_data['title']}\\n\")\n",
    "        md.write(f\"_Source: [{article_data['url']}]({article_data['url']})_\\n\\n\")\n",
    "\n",
    "        if article_data[\"intro\"]:\n",
    "            md.write(\"## Introduction\\n\")\n",
    "            for line in article_data[\"intro\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"• \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")\n",
    "\n",
    "        for sec in article_data[\"sections\"]:\n",
    "            md.write(f\"## {sec['heading']}\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"• \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600c800",
   "metadata": {},
   "source": [
    "## 2. Bulk Article URL Scrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19abbc2",
   "metadata": {},
   "source": [
    "#### Tool 1: Extract Links from Category page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c70a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 21 article URLs from <h4> tags.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_blog_urls(html_path, base_url=\"https://www.myupchar.com\"):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(html_path, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    for h4 in soup.find_all(\"h4\"):\n",
    "        a_tag = h4.find(\"a\", href=True)\n",
    "        if a_tag:\n",
    "            href = a_tag[\"href\"]\n",
    "            links.add(base_url + href.strip())\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "urls = extract_blog_urls(input(\"Paste category page URL here: \"))\n",
    "\n",
    "with open(\"myupchar_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(urls)} article URLs from <h4> tags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7091bf",
   "metadata": {},
   "source": [
    "#### Tool 2: Bulk Scrap articles from saved links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d14cc11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/21] Scraping: https://www.myupchar.com/pregnancy/which-side-is-baby-boy-in-womb\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/which-side-is-baby-boy-in-womb.txt\n",
      "  JSON: jsonblogs/pregnancy/which-side-is-baby-boy-in-womb.json\n",
      "  MD: markdownblogs/pregnancy/which-side-is-baby-boy-in-womb.md\n",
      "✅ Success: प्रेगनेंसी में बेबी बॉय किस साइड होता है? - Which side is baby boy side in womb in Hindi\n",
      "[2/21] Scraping: https://www.myupchar.com/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe\n",
      "✅ Saved:\n",
      "  TXT: blogs/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe.txt\n",
      "  JSON: jsonblogs/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe.json\n",
      "  MD: markdownblogs/sexual-health/masturbation-hastmaithun-in-hindi/masturbating-while-pregnant-is-safe.md\n",
      "✅ Success: क्या गर्भावस्था में हस्तमैथुन कर सकते हैं? - Is masturbation during pregnancy safe in Hindi\n",
      "[3/21] Scraping: https://www.myupchar.com/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/diet/what-should-eat-in-eighth-month-pregnancy.md\n",
      "✅ Success: गर्भवती महिला 8वें महीने में क्या खाए? - What Should Eat In Eighth Month Pregnancy In Hindi\n",
      "[4/21] Scraping: https://www.myupchar.com/pregnancy/can-you-get-pregnant-without-having-sex\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/can-you-get-pregnant-without-having-sex.txt\n",
      "  JSON: jsonblogs/pregnancy/can-you-get-pregnant-without-having-sex.json\n",
      "  MD: markdownblogs/pregnancy/can-you-get-pregnant-without-having-sex.md\n",
      "✅ Success: क्या बिना सेक्स के गर्भावस्था संभव है? - Can You Get Pregnant Without Having Sex In Hindi\n",
      "[5/21] Scraping: https://www.myupchar.com/pregnancy/effects-of-lack-of-sleep-during-pregnancy\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/effects-of-lack-of-sleep-during-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/effects-of-lack-of-sleep-during-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/effects-of-lack-of-sleep-during-pregnancy.md\n",
      "✅ Success: गर्भावस्था में कम सोने के दुष्प्रभाव - Effects Of Lack Of Sleep During Pregnancy In Hindi\n",
      "[6/21] Scraping: https://www.myupchar.com/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms.txt\n",
      "  JSON: jsonblogs/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms.json\n",
      "  MD: markdownblogs/pregnancy/muscular-dystrophy-affects-on-pregnancy-and-fetus-and-symptoms.md\n",
      "✅ Success: मस्कुलर डिस्ट्रॉफी और प्रेगनेंसी  - Muscular Dystrophy And Pregnancy In Hindi\n",
      "[7/21] Scraping: https://www.myupchar.com/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth.txt\n",
      "  JSON: jsonblogs/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth.json\n",
      "  MD: markdownblogs/pregnancy/normal-delivery/in-how-many-weeks-can-you-safely-give-birth.md\n",
      "✅ Success: कितने हफ्ते में डिलीवरी होनी चाहिए? - At how many weeks can you safely give birth in Hindi\n",
      "[8/21] Scraping: https://www.myupchar.com/pregnancy/vacuum-assisted-delivery\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/vacuum-assisted-delivery.txt\n",
      "  JSON: jsonblogs/pregnancy/vacuum-assisted-delivery.json\n",
      "  MD: markdownblogs/pregnancy/vacuum-assisted-delivery.md\n",
      "✅ Success: वैक्यूम असिस्टेड डिलीवरी - Know About Vacuum Assisted Delivery In Hindi\n",
      "[9/21] Scraping: https://www.myupchar.com/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein.txt\n",
      "  JSON: jsonblogs/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein.json\n",
      "  MD: markdownblogs/pregnancy/delivery-ke-baad-pet-ka-kalapan-kaise-door-karein.md\n",
      "✅ Success: डिलीवरी के बाद पेट का कालापन कैसे दूर करें? - how to get rid of blackness on stomach after delivery in hindi\n",
      "[10/21] Scraping: https://www.myupchar.com/pregnancy/pregnancy-brain\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/pregnancy-brain.txt\n",
      "  JSON: jsonblogs/pregnancy/pregnancy-brain.json\n",
      "  MD: markdownblogs/pregnancy/pregnancy-brain.md\n",
      "✅ Success: प्रेगनेंसी ब्रेन : क्या है, लक्षण, कारण व समाधान - Pregnancy Brain : What Is It, Symptoms, Causes And Treatment In Hindi\n",
      "[11/21] Scraping: https://www.myupchar.com/pregnancy/heart-diseases\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/heart-diseases.txt\n",
      "  JSON: jsonblogs/pregnancy/heart-diseases.json\n",
      "  MD: markdownblogs/pregnancy/heart-diseases.md\n",
      "✅ Success: प्रेगनेंसी में होने वाले हृदय रोग व लक्षण - Heart Diseases In Pregnancy And Symptoms In Hindi\n",
      "[12/21] Scraping: https://www.myupchar.com/pregnancy/how-to-prevent-blood-clots\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/how-to-prevent-blood-clots.txt\n",
      "  JSON: jsonblogs/pregnancy/how-to-prevent-blood-clots.json\n",
      "  MD: markdownblogs/pregnancy/how-to-prevent-blood-clots.md\n",
      "✅ Success: गर्भावस्था में खून के थक्के बनने से कैसे रोकें? - How To Prevent Blood Clots In Pregnancy In Hindi\n",
      "[13/21] Scraping: https://www.myupchar.com/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant.txt\n",
      "  JSON: jsonblogs/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant.json\n",
      "  MD: markdownblogs/pregnancy/jaldi-pregnant-hone-ke-tips-in-hindi/best-sex-position-to-get-pregnant.md\n",
      "✅ Success: प्रेग्नेंट होने के लिए बेस्ट सेक्स पोजीशन - Best sex position to get pregnant in Hindi\n",
      "[14/21] Scraping: https://www.myupchar.com/women-health/copper-t-in-hindi\n",
      "✅ Saved:\n",
      "  TXT: blogs/women-health/copper-t-in-hindi.txt\n",
      "  JSON: jsonblogs/women-health/copper-t-in-hindi.json\n",
      "  MD: markdownblogs/women-health/copper-t-in-hindi.md\n",
      "✅ Success: कॉपर टी क्यों, कैसे लगती है, निकलने का तरीका, फायदे और नुकसान - Copper T In Hindi\n",
      "[15/21] Scraping: https://www.myupchar.com/pregnancy/anembryonic-pregnancy\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/anembryonic-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/anembryonic-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/anembryonic-pregnancy.md\n",
      "✅ Success: एनब्रायोनिक गर्भावस्था - Anembryonic Pregnancy in Hindi\n",
      "[16/21] Scraping: https://www.myupchar.com/pregnancy/prega-news-in-hindi\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/prega-news-in-hindi.txt\n",
      "  JSON: jsonblogs/pregnancy/prega-news-in-hindi.json\n",
      "  MD: markdownblogs/pregnancy/prega-news-in-hindi.md\n",
      "✅ Success: प्रेगा न्यूज़ किट का उपयोग कैसे करें? - How To Use Prega News Pregnancy Test Kit In Hindi\n",
      "[17/21] Scraping: https://www.myupchar.com/pregnancy/diet/watermelon-for-pregnancy\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/diet/watermelon-for-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/diet/watermelon-for-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/diet/watermelon-for-pregnancy.md\n",
      "✅ Success: प्रेग्नेंसी में तरबूज खाने के फायदे  - Watermelon Benefits For Pregnancy in Hindi\n",
      "[18/21] Scraping: https://www.myupchar.com/surgery/c-section-cesarean-section/safe-sex-position-after-c-section\n",
      "✅ Saved:\n",
      "  TXT: blogs/surgery/c-section-cesarean-section/safe-sex-position-after-c-section.txt\n",
      "  JSON: jsonblogs/surgery/c-section-cesarean-section/safe-sex-position-after-c-section.json\n",
      "  MD: markdownblogs/surgery/c-section-cesarean-section/safe-sex-position-after-c-section.md\n",
      "✅ Success: सी-सेक्शन के बाद सेफ सेक्स पोजीशन - Safe Sex Position After Cesarean Delivery In Hindi\n",
      "[19/21] Scraping: https://www.myupchar.com/pregnancy/pregnancy-hip-pain\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/pregnancy-hip-pain.txt\n",
      "  JSON: jsonblogs/pregnancy/pregnancy-hip-pain.json\n",
      "  MD: markdownblogs/pregnancy/pregnancy-hip-pain.md\n",
      "✅ Success: गर्भावस्‍था में कूल्हों में दर्द - Hip Pain During Pregnancy in Hindi\n",
      "[20/21] Scraping: https://www.myupchar.com/pregnancy/when-a-baby-drops-during-pregnancy\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/when-a-baby-drops-during-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/when-a-baby-drops-during-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/when-a-baby-drops-during-pregnancy.md\n",
      "✅ Success: गर्भावस्था में शिशु नीचे हो तो क्या करें? - When A Baby Drops During Pregnancy In Hindi\n",
      "[21/21] Scraping: https://www.myupchar.com/pregnancy/diet/probiotics-for-pregnancy\n",
      "✅ Saved:\n",
      "  TXT: blogs/pregnancy/diet/probiotics-for-pregnancy.txt\n",
      "  JSON: jsonblogs/pregnancy/diet/probiotics-for-pregnancy.json\n",
      "  MD: markdownblogs/pregnancy/diet/probiotics-for-pregnancy.md\n",
      "✅ Success: क्या गर्भावस्था के दौरान प्रोबायोटिक्स लेना चाहिए? - Should Probiotics Be Taken During Pregnancy in Hindi\n",
      "📚 Master index saved to 'master_index.json'\n",
      "\n",
      "🟢 Finished: 21 success, 0 failed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Reuse the existing scrape_myupchar_article() and save_as_markdown() functions\n",
    "# Make sure those are defined/imported in this script\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def safe_path_from_url(url):\n",
    "    \"\"\"Convert the URL path into a folder path (no leading slash).\"\"\"\n",
    "    path = url.replace(\"https://www.myupchar.com/\", \"\").strip(\"/\")\n",
    "    return path  # e.g., \"en/disease/acidity\"\n",
    "\n",
    "\n",
    "def save_all_formats(article):\n",
    "    relative_path = safe_path_from_url(article[\"url\"])  # e.g., en/disease/acidity\n",
    "    base_name = os.path.basename(relative_path)  # e.g., acidity\n",
    "\n",
    "    # Paths\n",
    "    txt_path = os.path.join(\"blogs\", relative_path + \".txt\")\n",
    "    json_path = os.path.join(\"jsonblogs\", relative_path + \".json\")\n",
    "    md_path = os.path.join(\"markdownblogs\", relative_path + \".md\")\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(os.path.dirname(txt_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(md_path), exist_ok=True)\n",
    "\n",
    "    # Save as .txt\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Title: {article['title']}\\n\")\n",
    "        f.write(f\"URL: {article['url']}\\n\\n\")\n",
    "        f.write(\"== Introduction ==\\n\")\n",
    "        for line in article[\"intro\"]:\n",
    "            f.write(f\"{line}\\n\")\n",
    "        for sec in article[\"sections\"]:\n",
    "            f.write(f\"\\n== {sec['heading']} ==\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    # Save as .json\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Save as .md\n",
    "    save_as_markdown(article, md_path)\n",
    "\n",
    "    print(f\"✅ Saved:\\n  TXT: {txt_path}\\n  JSON: {json_path}\\n  MD: {md_path}\")\n",
    "\n",
    "\n",
    "def bulk_scrape(url_list, delay_range=(2, 5)):\n",
    "    failed_urls = []\n",
    "    success_index = []\n",
    "\n",
    "    for idx, url in enumerate(url_list, start=1):\n",
    "        print(f\"[{idx}/{len(url_list)}] Scraping: {url}\")\n",
    "\n",
    "        try:\n",
    "            # Use URL path to construct relative storage path\n",
    "            relative_path = safe_path_from_url(url)  # e.g., en/disease/acidity\n",
    "            base_name = os.path.basename(relative_path)\n",
    "\n",
    "            json_path = os.path.join(\"jsonblogs\", relative_path + \".json\")\n",
    "\n",
    "            # ✅ Skip early if already exists\n",
    "            if os.path.exists(json_path):\n",
    "                print(f\"⚠️ Skipped (already scraped): {relative_path}\")\n",
    "                continue\n",
    "\n",
    "            article = scrape_myupchar_article(url)\n",
    "            save_all_formats(article)\n",
    "            print(f\"✅ Success: {article['title']}\")\n",
    "\n",
    "            success_index.append(\n",
    "                {\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"slug\": base_name,\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"path\": relative_path,\n",
    "                    \"files\": {\n",
    "                        \"txt\": f\"blogs/{relative_path}.txt\",\n",
    "                        \"json\": f\"jsonblogs/{relative_path}.json\",\n",
    "                        \"md\": f\"markdownblogs/{relative_path}.md\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # ✅ Only wait if we actually scraped\n",
    "            time.sleep(random.uniform(*delay_range))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed: {url}\\n   Reason: {e}\")\n",
    "            failed_urls.append(url)\n",
    "\n",
    "    # Save failed URLs\n",
    "    if failed_urls:\n",
    "        with open(\"failed_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(url + \"\\n\" for url in failed_urls)\n",
    "        print(\"⚠️ Failed URLs saved to 'failed_urls.txt'\")\n",
    "\n",
    "    # Save master index\n",
    "    with open(\"master_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(success_index, f, indent=4, ensure_ascii=False)\n",
    "    print(\"📚 Master index saved to 'master_index.json'\")\n",
    "\n",
    "    print(f\"\\n🟢 Finished: {len(success_index)} success, {len(failed_urls)} failed.\")\n",
    "\n",
    "\n",
    "# Load main list of URLs\n",
    "with open(\"myupchar_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Load failed URLs (if resume is enabled)\n",
    "if os.path.exists(\"failed_urls.txt\"):\n",
    "    with open(\"failed_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        failed = [line.strip() for line in f if line.strip()]\n",
    "    all_urls.extend(failed)\n",
    "\n",
    "# Remove duplicates\n",
    "all_urls = list(set(all_urls))\n",
    "\n",
    "# Start scraper\n",
    "bulk_scrape(all_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
