{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84331c10",
   "metadata": {},
   "source": [
    "# MyUpchar blogs extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4580b",
   "metadata": {},
   "source": [
    "## 1. Article scrapper code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acdc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json, os\n",
    "\n",
    "\n",
    "def clean_text_with_links(tag):\n",
    "    \"\"\"Rebuild paragraph text while preserving <a> tag text with space.\"\"\"\n",
    "    parts = []\n",
    "    for child in tag.descendants:\n",
    "        if child.name == \"a\":\n",
    "            text = child.get_text(strip=True)\n",
    "            parts.append(text)\n",
    "        elif isinstance(child, str):\n",
    "            parts.append(child)\n",
    "    return \"\".join(parts).strip()\n",
    "\n",
    "\n",
    "def extract_intro_content(content_body_div):\n",
    "    \"\"\"Extracts the content before sections (main body / intro).\"\"\"\n",
    "    intro = []\n",
    "    for child in content_body_div.children:\n",
    "        if getattr(child, \"name\", None) == \"p\":\n",
    "            strong = child.find(\"strong\")\n",
    "            if strong and strong.text.strip() == child.text.strip():\n",
    "                intro.append(\"### \" + strong.get_text(strip=True))\n",
    "            else:\n",
    "                intro.append(clean_text_with_links(child))\n",
    "        elif getattr(child, \"name\", None) == \"ul\":\n",
    "            for li in child.find_all(\"li\"):\n",
    "                intro.append(\"• \" + clean_text_with_links(li))\n",
    "    return intro\n",
    "\n",
    "\n",
    "def scrape_myupchar_article(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"No Title\"\n",
    "    content = {\"url\": url, \"title\": title, \"intro\": [], \"sections\": []}\n",
    "\n",
    "    # === Extract Main Body (before sections)\n",
    "    intro_div = soup.find(\"div\", id=\"content-body\")\n",
    "    if intro_div:\n",
    "        content[\"intro\"] = extract_intro_content(intro_div)\n",
    "\n",
    "    # === Extract Structured Sections\n",
    "    for section in soup.select(\".category_blue_backdiv\"):\n",
    "        heading_tag = section.find(\"h2\")\n",
    "        body_tag = section.find(\"div\", class_=\"description\")\n",
    "\n",
    "        section_title = (\n",
    "            heading_tag.get_text(strip=True) if heading_tag else \"No Heading\"\n",
    "        )\n",
    "        section_text = []\n",
    "\n",
    "        if body_tag:\n",
    "            for child in body_tag.children:\n",
    "                if child.name == \"p\":\n",
    "                    strong = child.find(\"strong\")\n",
    "                    if strong and strong.text.strip() == child.text.strip():\n",
    "                        section_text.append(\"### \" + strong.get_text(strip=True))\n",
    "                    else:\n",
    "                        section_text.append(clean_text_with_links(child))\n",
    "                elif child.name == \"ul\":\n",
    "                    for li in child.find_all(\"li\"):\n",
    "                        section_text.append(\"• \" + clean_text_with_links(li))\n",
    "                elif child.name == \"div\":\n",
    "                    for subchild in child.children:\n",
    "                        if subchild.name == \"p\":\n",
    "                            section_text.append(clean_text_with_links(subchild))\n",
    "                        elif subchild.name == \"ul\":\n",
    "                            for li in subchild.find_all(\"li\"):\n",
    "                                section_text.append(\"• \" + clean_text_with_links(li))\n",
    "\n",
    "        content[\"sections\"].append({\"heading\": section_title, \"body\": section_text})\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab83d1f",
   "metadata": {},
   "source": [
    "### 1.1. Save as Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e368ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_markdown(article_data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as md:\n",
    "        md.write(f\"# {article_data['title']}\\n\")\n",
    "        md.write(f\"_Source: [{article_data['url']}]({article_data['url']})_\\n\\n\")\n",
    "\n",
    "        if article_data[\"intro\"]:\n",
    "            md.write(\"## Introduction\\n\")\n",
    "            for line in article_data[\"intro\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"• \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")\n",
    "\n",
    "        for sec in article_data[\"sections\"]:\n",
    "            md.write(f\"## {sec['heading']}\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"• \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600c800",
   "metadata": {},
   "source": [
    "## 2. Bulk Article URL Scrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19abbc2",
   "metadata": {},
   "source": [
    "#### Tool 1: Extract Links from Category page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c70a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_blog_urls(html_path, base_url=\"https://www.myupchar.com\"):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(html_path, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    for h4 in soup.find_all(\"h4\"):\n",
    "        a_tag = h4.find(\"a\", href=True)\n",
    "        if a_tag:\n",
    "            href = a_tag[\"href\"]\n",
    "            links.add(base_url + href.strip())\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "urls = extract_blog_urls(input(\"Paste category page URL here: \"))\n",
    "\n",
    "with open(\"myupchar_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(urls)} article URLs from <h4> tags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7091bf",
   "metadata": {},
   "source": [
    "#### Tool 2: Bulk Scrap articles from saved links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d14cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Reuse the existing scrape_myupchar_article() and save_as_markdown() functions\n",
    "# Make sure those are defined/imported in this script\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "def safe_path_from_url(url):\n",
    "    \"\"\"Convert the URL path into a folder path (no leading slash).\"\"\"\n",
    "    path = url.replace(\"https://www.myupchar.com/\", \"\").strip(\"/\")\n",
    "    return path  # e.g., \"en/disease/acidity\"\n",
    "\n",
    "\n",
    "def save_all_formats(article):\n",
    "    relative_path = safe_path_from_url(article[\"url\"])  # e.g., en/disease/acidity\n",
    "    base_name = os.path.basename(relative_path)  # e.g., acidity\n",
    "\n",
    "    # Paths\n",
    "    txt_path = os.path.join(\"blogs\", relative_path + \".txt\")\n",
    "    json_path = os.path.join(\"jsonblogs\", relative_path + \".json\")\n",
    "    md_path = os.path.join(\"markdownblogs\", relative_path + \".md\")\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(os.path.dirname(txt_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(json_path), exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(md_path), exist_ok=True)\n",
    "\n",
    "    # Save as .txt\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Title: {article['title']}\\n\")\n",
    "        f.write(f\"URL: {article['url']}\\n\\n\")\n",
    "        f.write(\"== Introduction ==\\n\")\n",
    "        for line in article[\"intro\"]:\n",
    "            f.write(f\"{line}\\n\")\n",
    "        for sec in article[\"sections\"]:\n",
    "            f.write(f\"\\n== {sec['heading']} ==\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    # Save as .json\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Save as .md\n",
    "    save_as_markdown(article, md_path)\n",
    "\n",
    "    print(f\"✅ Saved:\\n  TXT: {txt_path}\\n  JSON: {json_path}\\n  MD: {md_path}\")\n",
    "\n",
    "\n",
    "def bulk_scrape(url_list, delay_range=(2, 5)):\n",
    "    failed_urls = []\n",
    "    success_index = []\n",
    "\n",
    "    for idx, url in enumerate(url_list, start=1):\n",
    "        print(f\"[{idx}/{len(url_list)}] Scraping: {url}\")\n",
    "\n",
    "        try:\n",
    "            # Use URL path to construct relative storage path\n",
    "            relative_path = safe_path_from_url(url)  # e.g., en/disease/acidity\n",
    "            base_name = os.path.basename(relative_path)\n",
    "\n",
    "            json_path = os.path.join(\"jsonblogs\", relative_path + \".json\")\n",
    "\n",
    "            # ✅ Skip early if already exists\n",
    "            if os.path.exists(json_path):\n",
    "                print(f\"⚠️ Skipped (already scraped): {relative_path}\")\n",
    "                continue\n",
    "\n",
    "            article = scrape_myupchar_article(url)\n",
    "            save_all_formats(article)\n",
    "            print(f\"✅ Success: {article['title']}\")\n",
    "\n",
    "            success_index.append(\n",
    "                {\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"slug\": base_name,\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"path\": relative_path,\n",
    "                    \"files\": {\n",
    "                        \"txt\": f\"blogs/{relative_path}.txt\",\n",
    "                        \"json\": f\"jsonblogs/{relative_path}.json\",\n",
    "                        \"md\": f\"markdownblogs/{relative_path}.md\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # ✅ Only wait if we actually scraped\n",
    "            time.sleep(random.uniform(*delay_range))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed: {url}\\n   Reason: {e}\")\n",
    "            failed_urls.append(url)\n",
    "\n",
    "    # Save failed URLs\n",
    "    if failed_urls:\n",
    "        with open(\"failed_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(url + \"\\n\" for url in failed_urls)\n",
    "        print(\"⚠️ Failed URLs saved to 'failed_urls.txt'\")\n",
    "\n",
    "    # Save master index\n",
    "    with open(\"master_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(success_index, f, indent=4, ensure_ascii=False)\n",
    "    print(\"📚 Master index saved to 'master_index.json'\")\n",
    "\n",
    "    print(f\"\\n🟢 Finished: {len(success_index)} success, {len(failed_urls)} failed.\")\n",
    "\n",
    "\n",
    "# Load main list of URLs\n",
    "with open(\"myupchar_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Load failed URLs (if resume is enabled)\n",
    "if os.path.exists(\"failed_urls.txt\"):\n",
    "    with open(\"failed_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        failed = [line.strip() for line in f if line.strip()]\n",
    "    all_urls.extend(failed)\n",
    "\n",
    "# Remove duplicates\n",
    "all_urls = list(set(all_urls))\n",
    "\n",
    "# Start scraper\n",
    "bulk_scrape(all_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
