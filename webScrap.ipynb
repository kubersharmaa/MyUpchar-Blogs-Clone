{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84331c10",
   "metadata": {},
   "source": [
    "# MyUpchar blogs extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc4580b",
   "metadata": {},
   "source": [
    "## 1. Article scrapper code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3acdc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json, os\n",
    "\n",
    "\n",
    "def clean_text_with_links(tag):\n",
    "    \"\"\"Rebuild paragraph text while preserving <a> tag text with space.\"\"\"\n",
    "    parts = []\n",
    "    for child in tag.descendants:\n",
    "        if child.name == \"a\":\n",
    "            text = child.get_text(strip=True)\n",
    "            parts.append(text)\n",
    "        elif isinstance(child, str):\n",
    "            parts.append(child)\n",
    "    return \"\".join(parts).strip()\n",
    "\n",
    "\n",
    "def extract_intro_content(content_body_div):\n",
    "    \"\"\"Extracts the content before sections (main body / intro).\"\"\"\n",
    "    intro = []\n",
    "    for child in content_body_div.children:\n",
    "        if getattr(child, \"name\", None) == \"p\":\n",
    "            strong = child.find(\"strong\")\n",
    "            if strong and strong.text.strip() == child.text.strip():\n",
    "                intro.append(\"### \" + strong.get_text(strip=True))\n",
    "            else:\n",
    "                intro.append(clean_text_with_links(child))\n",
    "        elif getattr(child, \"name\", None) == \"ul\":\n",
    "            for li in child.find_all(\"li\"):\n",
    "                intro.append(\"• \" + clean_text_with_links(li))\n",
    "    return intro\n",
    "\n",
    "\n",
    "def scrape_myupchar_article(url):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    title = soup.find(\"h1\").get_text(strip=True) if soup.find(\"h1\") else \"No Title\"\n",
    "    content = {\"url\": url, \"title\": title, \"intro\": [], \"sections\": []}\n",
    "\n",
    "    # === Extract Main Body (before sections)\n",
    "    intro_div = soup.find(\"div\", id=\"content-body\")\n",
    "    if intro_div:\n",
    "        content[\"intro\"] = extract_intro_content(intro_div)\n",
    "\n",
    "    # === Extract Structured Sections\n",
    "    for section in soup.select(\".category_blue_backdiv\"):\n",
    "        heading_tag = section.find(\"h2\")\n",
    "        body_tag = section.find(\"div\", class_=\"description\")\n",
    "\n",
    "        section_title = (\n",
    "            heading_tag.get_text(strip=True) if heading_tag else \"No Heading\"\n",
    "        )\n",
    "        section_text = []\n",
    "\n",
    "        if body_tag:\n",
    "            for child in body_tag.children:\n",
    "                if child.name == \"p\":\n",
    "                    strong = child.find(\"strong\")\n",
    "                    if strong and strong.text.strip() == child.text.strip():\n",
    "                        section_text.append(\"### \" + strong.get_text(strip=True))\n",
    "                    else:\n",
    "                        section_text.append(clean_text_with_links(child))\n",
    "                elif child.name == \"ul\":\n",
    "                    for li in child.find_all(\"li\"):\n",
    "                        section_text.append(\"• \" + clean_text_with_links(li))\n",
    "                elif child.name == \"div\":\n",
    "                    for subchild in child.children:\n",
    "                        if subchild.name == \"p\":\n",
    "                            section_text.append(clean_text_with_links(subchild))\n",
    "                        elif subchild.name == \"ul\":\n",
    "                            for li in subchild.find_all(\"li\"):\n",
    "                                section_text.append(\"• \" + clean_text_with_links(li))\n",
    "\n",
    "        content[\"sections\"].append({\"heading\": section_title, \"body\": section_text})\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab83d1f",
   "metadata": {},
   "source": [
    "### 1.1. Save as Markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e368ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_markdown(article_data, filename):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as md:\n",
    "        md.write(f\"# {article_data['title']}\\n\")\n",
    "        md.write(f\"_Source: [{article_data['url']}]({article_data['url']})_\\n\\n\")\n",
    "\n",
    "        if article_data[\"intro\"]:\n",
    "            md.write(\"## Introduction\\n\")\n",
    "            for line in article_data[\"intro\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"• \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")\n",
    "\n",
    "        for sec in article_data[\"sections\"]:\n",
    "            md.write(f\"## {sec['heading']}\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                if line.startswith(\"### \"):\n",
    "                    md.write(f\"{line}\\n\")\n",
    "                elif line.startswith(\"• \"):\n",
    "                    md.write(f\"- {line[2:]}\\n\")\n",
    "                else:\n",
    "                    md.write(f\"{line}\\n\")\n",
    "            md.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600c800",
   "metadata": {},
   "source": [
    "### 1.2. Single URL Scrapper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f69ff65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved as:\n",
      "- blogs/acidity.txt\n",
      "- jsonblogs/acidity.json\n",
      "- markdownblogs/acidity.md\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.myupchar.com/en/disease/acidity\"\n",
    "article = scrape_myupchar_article(url)\n",
    "\n",
    "# File-safe title\n",
    "safe_title = article[\"title\"].lower().replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "# Create folders if not exist\n",
    "os.makedirs(\"blogs\", exist_ok=True)\n",
    "os.makedirs(\"jsonblogs\", exist_ok=True)\n",
    "os.makedirs(\"markdownblogs\", exist_ok=True)\n",
    "\n",
    "# 1. Save as readable text\n",
    "text_filename = f\"blogs/{safe_title}.txt\"\n",
    "with open(text_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"Title: {article['title']}\\n\")\n",
    "    f.write(f\"URL: {article['url']}\\n\\n\")\n",
    "    f.write(\"== Introduction ==\\n\")\n",
    "    for line in article[\"intro\"]:\n",
    "        f.write(f\"{line}\\n\")\n",
    "\n",
    "    for sec in article[\"sections\"]:\n",
    "        f.write(f\"\\n== {sec['heading']} ==\\n\")\n",
    "        for line in sec[\"body\"]:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "# 2. Save as structured JSON\n",
    "json_filename = f\"jsonblogs/{safe_title}.json\"\n",
    "with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(article, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# 3. Save as markdown\n",
    "md_filename = f\"markdownblogs/{safe_title}.md\"\n",
    "save_as_markdown(article, md_filename)\n",
    "\n",
    "print(f\"✅ Saved as:\\n- {text_filename}\\n- {json_filename}\\n- {md_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245b17c4",
   "metadata": {},
   "source": [
    "### 1.3. Bulk URL Scrapper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19abbc2",
   "metadata": {},
   "source": [
    "#### 1.3.1. Extract Links from Category page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c70a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Extracted 85 article URLs from <h4> tags.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def extract_blog_urls(html_path, base_url=\"https://www.myupchar.com\"):\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(html_path, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    links = set()\n",
    "\n",
    "    for h4 in soup.find_all(\"h4\"):\n",
    "        a_tag = h4.find(\"a\", href=True)\n",
    "        if a_tag:\n",
    "            href = a_tag[\"href\"]\n",
    "            if href.startswith(\"/en/\"):\n",
    "                links.add(base_url + href.strip())\n",
    "\n",
    "    return sorted(links)\n",
    "\n",
    "\n",
    "urls = extract_blog_urls(\n",
    "    \"https://www.myupchar.com/en/disease/stomach-liver-mouth-and-digestion-related-diseases\"\n",
    ")\n",
    "\n",
    "with open(\"myupchar_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for url in urls:\n",
    "        f.write(url + \"\\n\")\n",
    "\n",
    "print(f\"✅ Extracted {len(urls)} article URLs from <h4> tags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7091bf",
   "metadata": {},
   "source": [
    "#### 1.3.2. Bulk Scrap articles from saved links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d14cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# Reuse the existing scrape_myupchar_article() and save_as_markdown() functions\n",
    "# Make sure those are defined/imported in this script\n",
    "\n",
    "\n",
    "def save_all_formats(article):\n",
    "    safe_title = article[\"title\"].lower().replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "    # Ensure output folders exist\n",
    "    os.makedirs(\"blogs\", exist_ok=True)\n",
    "    os.makedirs(\"jsonblogs\", exist_ok=True)\n",
    "    os.makedirs(\"markdownblogs\", exist_ok=True)\n",
    "\n",
    "    # Save .txt\n",
    "    with open(f\"blogs/{safe_title}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"Title: {article['title']}\\n\")\n",
    "        f.write(f\"URL: {article['url']}\\n\\n\")\n",
    "        f.write(\"== Introduction ==\\n\")\n",
    "        for line in article[\"intro\"]:\n",
    "            f.write(f\"{line}\\n\")\n",
    "        for sec in article[\"sections\"]:\n",
    "            f.write(f\"\\n== {sec['heading']} ==\\n\")\n",
    "            for line in sec[\"body\"]:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    # Save .json\n",
    "    with open(f\"jsonblogs/{safe_title}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(article, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    # Save .md\n",
    "    save_as_markdown(article, f\"markdownblogs/{safe_title}.md\")\n",
    "\n",
    "\n",
    "def bulk_scrape(url_list, delay_range=(2, 5), resume_failed=True):\n",
    "    failed_urls = []\n",
    "    success_index = []\n",
    "\n",
    "    for idx, url in enumerate(url_list, start=1):\n",
    "        print(f\"[{idx}/{len(url_list)}] Scraping: {url}\")\n",
    "        try:\n",
    "            article = scrape_myupchar_article(url)\n",
    "            safe_title = article[\"title\"].lower().replace(\" \", \"_\").replace(\"/\", \"_\")\n",
    "\n",
    "            # Skip if already exists\n",
    "            if os.path.exists(f\"jsonblogs/{safe_title}.json\"):\n",
    "                print(f\"⚠️ Skipped (already scraped): {article['title']}\")\n",
    "                continue\n",
    "\n",
    "            save_all_formats(article)\n",
    "            print(f\"✅ Success: {article['title']}\")\n",
    "\n",
    "            # Add to master index\n",
    "            success_index.append(\n",
    "                {\n",
    "                    \"title\": article[\"title\"],\n",
    "                    \"slug\": safe_title,\n",
    "                    \"url\": article[\"url\"],\n",
    "                    \"files\": {\n",
    "                        \"txt\": f\"blogs/{safe_title}.txt\",\n",
    "                        \"json\": f\"jsonblogs/{safe_title}.json\",\n",
    "                        \"md\": f\"markdownblogs/{safe_title}.md\",\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed: {url}\\n   Reason: {e}\")\n",
    "            failed_urls.append(url)\n",
    "\n",
    "        time.sleep(random.uniform(*delay_range))\n",
    "\n",
    "    # Save failed URLs\n",
    "    if failed_urls:\n",
    "        with open(\"failed_urls.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            for url in failed_urls:\n",
    "                f.write(url + \"\\n\")\n",
    "        print(f\"⚠️ Failed URLs saved to 'failed_urls.txt'\")\n",
    "\n",
    "    # Save master index\n",
    "    with open(\"master_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(success_index, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"📚 Master index saved to 'master_index.json'\")\n",
    "\n",
    "    print(\n",
    "        f\"\\n🟢 Bulk scraping finished with {len(success_index)} success, {len(failed_urls)} failed.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9cb02c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/85] Scraping: https://www.myupchar.com/en/disease/fatty-liver\n",
      "✅ Success: Fatty Liver\n",
      "[2/85] Scraping: https://www.myupchar.com/en/disease/anal-fissure\n",
      "✅ Success: Anal Fissure\n",
      "[3/85] Scraping: https://www.myupchar.com/en/disease/colon-infection\n",
      "✅ Success: Colon infection\n",
      "[4/85] Scraping: https://www.myupchar.com/en/disease/dental-caries-cavities\n",
      "✅ Success: Cavities (Dental Caries)\n",
      "[5/85] Scraping: https://www.myupchar.com/en/disease/teeth-infections\n",
      "✅ Success: Teeth Infections\n",
      "[6/85] Scraping: https://www.myupchar.com/en/disease/appendicitis\n",
      "✅ Success: Appendicitis\n",
      "[7/85] Scraping: https://www.myupchar.com/en/disease/hernia\n",
      "✅ Success: Hernia\n",
      "[8/85] Scraping: https://www.myupchar.com/en/disease/mouth-ulcer\n",
      "✅ Success: Mouth ulcer\n",
      "[9/85] Scraping: https://www.myupchar.com/en/disease/hepatitis-c\n",
      "✅ Success: Hepatitis C\n",
      "[10/85] Scraping: https://www.myupchar.com/en/disease/liver-cirrhosis\n",
      "✅ Success: Liver Cirrhosis\n",
      "[11/85] Scraping: https://www.myupchar.com/en/disease/black-tongue\n",
      "✅ Success: Black Tongue\n",
      "[12/85] Scraping: https://www.myupchar.com/en/disease/gerd-gastroesophageal-reflux-disease\n",
      "✅ Success: GERD (Gastroesophageal Reflux Disease)\n",
      "[13/85] Scraping: https://www.myupchar.com/en/disease/white-coated-tongue\n",
      "✅ Success: White Coated Tongue\n",
      "[14/85] Scraping: https://www.myupchar.com/en/disease/hepatitis\n",
      "✅ Success: Hepatitis\n",
      "[15/85] Scraping: https://www.myupchar.com/en/disease/bruxism\n",
      "✅ Success: Bruxism (Teeth Grinding)\n",
      "[16/85] Scraping: https://www.myupchar.com/en/disease/swollen-gums\n",
      "✅ Success: Swollen gums\n",
      "[17/85] Scraping: https://www.myupchar.com/en/disease/pancreatitis\n",
      "✅ Success: Pancreatitis\n",
      "[18/85] Scraping: https://www.myupchar.com/en/disease/peritonitis\n",
      "✅ Success: Peritonitis\n",
      "[19/85] Scraping: https://www.myupchar.com/en/disease/hepatitis-a\n",
      "✅ Success: Hepatitis A\n",
      "[20/85] Scraping: https://www.myupchar.com/en/disease/gingivitis\n",
      "✅ Success: Gingivitis\n",
      "[21/85] Scraping: https://www.myupchar.com/en/disease/glossitis\n",
      "✅ Success: Swollen tongue\n",
      "[22/85] Scraping: https://www.myupchar.com/en/disease/blood-in-stool\n",
      "✅ Success: Blood in Stool\n",
      "[23/85] Scraping: https://www.myupchar.com/en/disease/constipation\n",
      "✅ Success: Constipation\n",
      "[24/85] Scraping: https://www.myupchar.com/en/disease/liver-failure\n",
      "✅ Success: Liver Failure\n",
      "[25/85] Scraping: https://www.myupchar.com/en/disease/necrotizing-enterocolitis\n",
      "✅ Success: Necrotizing Enterocolitis\n",
      "[26/85] Scraping: https://www.myupchar.com/en/disease/liver-infection\n",
      "✅ Success: Liver Infection\n",
      "[27/85] Scraping: https://www.myupchar.com/en/disease/liver-pain\n",
      "✅ Success: Liver pain\n",
      "[28/85] Scraping: https://www.myupchar.com/en/disease/tooth-plaque\n",
      "✅ Success: Dental Plaque\n",
      "[29/85] Scraping: https://www.myupchar.com/en/disease/polyphagia-increased-appetite\n",
      "✅ Success: Polyphagia or Increased Appetite\n",
      "[30/85] Scraping: https://www.myupchar.com/en/disease/primary-biliary-cholangitis-pbc\n",
      "✅ Success: Primary Biliary Cholangitis (PBC)\n",
      "[31/85] Scraping: https://www.myupchar.com/en/disease/peptic-ulcer\n",
      "✅ Success: Peptic Ulcer\n",
      "[32/85] Scraping: https://www.myupchar.com/en/disease/indigestion\n",
      "✅ Success: Indigestion\n",
      "[33/85] Scraping: https://www.myupchar.com/en/disease/hepatic-encephalopathy\n",
      "✅ Success: Hepatic Encephalopathy\n",
      "[34/85] Scraping: https://www.myupchar.com/en/disease/hepatitis-e\n",
      "✅ Success: Hepatitis E\n",
      "[35/85] Scraping: https://www.myupchar.com/en/disease/leukoplakia\n",
      "✅ Success: Leukoplakia\n",
      "[36/85] Scraping: https://www.myupchar.com/en/disease/tongue-ulcers\n",
      "✅ Success: Tongue Ulcers\n",
      "[37/85] Scraping: https://www.myupchar.com/en/disease/diarrhea\n",
      "✅ Success: Diarrhoea (Loose Motions)\n",
      "[38/85] Scraping: https://www.myupchar.com/en/disease/mucus-stool\n",
      "✅ Success: Mucus in stool\n",
      "[39/85] Scraping: https://www.myupchar.com/en/disease/burning-mouth-syndrome\n",
      "✅ Success: Burning Mouth Syndrome\n",
      "[40/85] Scraping: https://www.myupchar.com/en/disease/loose-tooth\n",
      "✅ Success: Loose tooth\n",
      "[41/85] Scraping: https://www.myupchar.com/en/disease/weak-digestion\n",
      "✅ Success: Weak Digestion\n",
      "[42/85] Scraping: https://www.myupchar.com/en/disease/gastritis\n",
      "✅ Success: Gastritis\n",
      "[43/85] Scraping: https://www.myupchar.com/en/disease/stomach-pain\n",
      "⚠️ Skipped (already scraped): Stomach ache\n",
      "[44/85] Scraping: https://www.myupchar.com/en/disease/hepatitis-b\n",
      "✅ Success: Hepatitis B\n",
      "[45/85] Scraping: https://www.myupchar.com/en/disease/rectal-prolapse\n",
      "✅ Success: Rectal Prolapse\n",
      "[46/85] Scraping: https://www.myupchar.com/en/disease/tropical-sprue\n",
      "✅ Success: Tropical Sprue\n",
      "[47/85] Scraping: https://www.myupchar.com/en/disease/sensitive-teeth\n",
      "✅ Success: Sensitive Teeth\n",
      "[48/85] Scraping: https://www.myupchar.com/en/disease/molar-tooth-pain\n",
      "✅ Success: Molar Tooth Pain\n",
      "[49/85] Scraping: https://www.myupchar.com/en/disease/gallbladder-stones\n",
      "✅ Success: Gallbladder Stones\n",
      "[50/85] Scraping: https://www.myupchar.com/en/disease/achalasia\n",
      "✅ Success: Achalasia\n",
      "[51/85] Scraping: https://www.myupchar.com/en/disease/gastroparesis\n",
      "✅ Success: Gastroparesis\n",
      "[52/85] Scraping: https://www.myupchar.com/en/disease/dysentery\n",
      "✅ Success: Dysentery\n",
      "[53/85] Scraping: https://www.myupchar.com/en/disease/malabsorption-syndrome\n",
      "✅ Success: Malabsorption Syndrome\n",
      "[54/85] Scraping: https://www.myupchar.com/en/disease/enlarged-liver\n",
      "✅ Success: Enlarged Liver\n",
      "[55/85] Scraping: https://www.myupchar.com/en/disease/acid-reflux\n",
      "✅ Success: Acid reflux\n",
      "[56/85] Scraping: https://www.myupchar.com/en/disease/cholestatic-liver-diseases\n",
      "✅ Success: Cholestatic Liver Diseases\n",
      "[57/85] Scraping: https://www.myupchar.com/en/disease/stomach-gas\n",
      "✅ Success: Stomach Gas\n",
      "[58/85] Scraping: https://www.myupchar.com/en/disease/wisdom-tooth-pain\n",
      "✅ Success: Wisdom Tooth Pain\n",
      "[59/85] Scraping: https://www.myupchar.com/en/disease/heartburn\n",
      "✅ Success: Heartburn\n",
      "[60/85] Scraping: https://www.myupchar.com/en/disease/paralytic-ileus\n",
      "✅ Success: Paralytic Ileus\n",
      "[61/85] Scraping: https://www.myupchar.com/en/disease/cholecystitis\n",
      "✅ Success: Cholecystitis\n",
      "[62/85] Scraping: https://www.myupchar.com/en/disease/anal-fistula\n",
      "✅ Success: Anal Fistula\n",
      "[63/85] Scraping: https://www.myupchar.com/en/disease/digestive-disorders\n",
      "✅ Success: Digestive Disorders\n",
      "[64/85] Scraping: https://www.myupchar.com/en/disease/paragonimiasis\n",
      "✅ Success: Paragonimiasis\n",
      "[65/85] Scraping: https://www.myupchar.com/en/disease/gum-disease-periodontitis\n",
      "✅ Success: Gum Disease (Periodontitis)\n",
      "[66/85] Scraping: https://www.myupchar.com/en/disease/toothache\n",
      "✅ Success: Toothache\n",
      "[67/85] Scraping: https://www.myupchar.com/en/disease/upset-stomach\n",
      "✅ Success: Upset Stomach\n",
      "[68/85] Scraping: https://www.myupchar.com/en/disease/irritable-bowel-syndrome\n",
      "✅ Success: Irritable Bowel Syndrome (IBS)\n",
      "[69/85] Scraping: https://www.myupchar.com/en/disease/rectal-bleeding\n",
      "✅ Success: Rectal bleeding\n",
      "[70/85] Scraping: https://www.myupchar.com/en/disease/weak-liver\n",
      "✅ Success: Weak Liver\n",
      "[71/85] Scraping: https://www.myupchar.com/en/disease/bad-breath\n",
      "⚠️ Skipped (already scraped): Bad Breath\n",
      "[72/85] Scraping: https://www.myupchar.com/en/disease/canker-sores\n",
      "✅ Success: Canker Sores\n",
      "[73/85] Scraping: https://www.myupchar.com/en/disease/ulcerative-colitis\n",
      "✅ Success: Ulcerative Colitis\n",
      "[74/85] Scraping: https://www.myupchar.com/en/disease/liver-disease\n",
      "✅ Success: Liver Disease\n",
      "[75/85] Scraping: https://www.myupchar.com/en/disease/hepatitis-d\n",
      "✅ Success: Hepatitis D\n",
      "[76/85] Scraping: https://www.myupchar.com/en/disease/gastroenteritis\n",
      "✅ Success: Stomach Infection\n",
      "[77/85] Scraping: https://www.myupchar.com/en/disease/jaundice\n",
      "⚠️ Skipped (already scraped): Jaundice\n",
      "[78/85] Scraping: https://www.myupchar.com/en/disease/liver-injury\n",
      "✅ Success: Liver Injury\n",
      "[79/85] Scraping: https://www.myupchar.com/en/disease/acid-reflux-babies-gerd\n",
      "✅ Success: Acid reflux in babies\n",
      "[80/85] Scraping: https://www.myupchar.com/en/disease/celiac-disease\n",
      "✅ Success: Celiac Disease\n",
      "[81/85] Scraping: https://www.myupchar.com/en/disease/crohns-disease\n",
      "✅ Success: Crohn's Disease\n",
      "[82/85] Scraping: https://www.myupchar.com/en/disease/dental-abscess\n",
      "✅ Success: Dental Abscess\n",
      "[83/85] Scraping: https://www.myupchar.com/en/disease/diverticulitis\n",
      "✅ Success: Diverticulitis\n",
      "[84/85] Scraping: https://www.myupchar.com/en/disease/bleeding-gums\n",
      "✅ Success: Bleeding Gums\n",
      "[85/85] Scraping: https://www.myupchar.com/en/disease/acidity\n",
      "⚠️ Skipped (already scraped): Acidity\n",
      "📚 Master index saved to 'master_index.json'\n",
      "\n",
      "🟢 Bulk scraping finished with 81 success, 0 failed.\n"
     ]
    }
   ],
   "source": [
    "# Load main list of URLs\n",
    "with open(\"myupchar_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    all_urls = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# Load failed URLs (if resume is enabled)\n",
    "if os.path.exists(\"failed_urls.txt\"):\n",
    "    with open(\"failed_urls.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        failed = [line.strip() for line in f if line.strip()]\n",
    "    all_urls.extend(failed)\n",
    "\n",
    "# Remove duplicates\n",
    "all_urls = list(set(all_urls))\n",
    "\n",
    "# Start scraper\n",
    "bulk_scrape(all_urls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
